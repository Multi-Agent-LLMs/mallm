from .metric import Metric
from rouge_score import rouge_scorer
from typing import Any


class ROUGE(Metric):
    """
    A class to evaluate the ROUGE score for text generation tasks.
    """

    def evaluate(
        self, generated_text: str, reference_texts: list[str]
    ) -> dict[str, Any]:
        """
        Evaluate the generated text against a reference text using ROUGE score.

        Args:
        generated_text (str): The text generated by the model.
        reference_text (str): The reference text to compare against.

        Returns:
        float: The ROUGE score.
        """
        # Calculate ROUGE score
        scorer = rouge_scorer.RougeScorer(
            ["rouge1", "rouge2", "rouge3", "rougeL"], use_stemmer=True
        )
        scores = scorer.score(
            generated_text, reference_texts[0]
        )  # rouge only takes one reference
        scores = {
            "rouge1": {
                "precision": scores["rouge1"].precision,
                "recall": scores["rouge1"].recall,
                "fmeasure": scores["rouge1"].fmeasure,
            },
            "rouge2": {
                "precision": scores["rouge2"].precision,
                "recall": scores["rouge2"].recall,
                "fmeasure": scores["rouge2"].fmeasure,
            },
            "rouge3": {
                "precision": scores["rouge3"].precision,
                "recall": scores["rouge3"].recall,
                "fmeasure": scores["rouge3"].fmeasure,
            },
            "rougeL": {
                "precision": scores["rougeL"].precision,
                "recall": scores["rougeL"].recall,
                "fmeasure": scores["rougeL"].fmeasure,
            },
        }
        return scores

    def get_metric_name(self) -> str:
        """
        Return the name of the evaluation metric.

        Returns:
        str: The name of the metric, "ROUGE".
        """
        return "ROUGE"
